{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\danie\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\danie\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\danie\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "#I installed the xgboost module with conda install -c conda-forge xgboost\n",
    "\n",
    "trainSet = pd.read_csv('TrainOnMe.csv',index_col=0 )\n",
    "evalSet = pd.read_csv('EvaluateOnMe.csv',index_col=0)\n",
    "\n",
    "#Clean trainingset\n",
    "trainSet.iat[208,12] = \"False\"\n",
    "trainSet = trainSet.dropna()\n",
    "\n",
    "\n",
    "trainSet = trainSet[trainSet.x6 != \"Bayesian Interference\"]\n",
    "trainSet = trainSet[trainSet.x12 != \"Flase\"]\n",
    "\n",
    "X_train = trainSet.iloc[:,1:13].values\n",
    "y_train = trainSet.iloc[:,0].values\n",
    "eval_train = evalSet.values\n",
    "x6 = trainSet.iloc[:,6].values\n",
    "x12 = trainSet.iloc[:,12].values\n",
    "#do the same for evalSet\n",
    "e_x6 = evalSet.iloc[:,5].values\n",
    "e_x12 = evalSet.iloc[:,11].values\n",
    "\n",
    "\n",
    "\n",
    "#convert y,x6, x12, e_x6 and e_x12 to numeric\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "le1.fit([\"GMMs and Accordions\", \"Bayesian Inference\"])\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2 = le2.fit([\"Shoogee\",\"Atsuto\", \"Bob\", \"Jorg\"])\n",
    "le3 = preprocessing.LabelEncoder()\n",
    "le3.fit([\"True\", \"False\"])\n",
    "e_le1 = preprocessing.LabelEncoder()\n",
    "e_le2 = preprocessing.LabelEncoder()\n",
    "e_le1.fit([\"GMMs and Accordions\", \"Bayesian Inference\"])\n",
    "e_le2.fit([True, False])\n",
    "e_x6 = e_le1.transform(e_x6)\n",
    "e_x12 = e_le2.transform(e_x12)\n",
    "eval_train[:,5] = e_x6\n",
    "eval_train[:,11] = e_x12\n",
    "x6 = le1.transform(x6)\n",
    "y_train = le2.transform(y_train) \n",
    "x12 = le3.transform(x12)\n",
    "X_train[:,5] = x6\n",
    "X_train[:,11] = x12\n",
    " \n",
    "estimator = XGBClassifier(\n",
    "                    seed=29,\n",
    "                    n_jobs=4,\n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators = 1000,\n",
    "                    max_depth = 5,\n",
    "                    min_child_weight=1,\n",
    "                   \n",
    "                    gamma=0,\n",
    "                   \n",
    "                    objective='multi:softmax',\n",
    "                    verbosity = 0,\n",
    "    \n",
    "                     subsample=0.8,\n",
    "                \n",
    "                    colsample_bytree=0.8\n",
    "                   \n",
    "                    )\n",
    "#perform cross valid. to determine n_estimators, defaults at 5-fold\n",
    "\n",
    "xgb_params = estimator.get_xgb_params()\n",
    "\n",
    "xgb_params['num_class']=4\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_train, label= y_train)\n",
    "\n",
    "cv = xgb.cv(xgb_params, xgb_train,\n",
    "            num_boost_round=estimator.get_params()['n_estimators'],\n",
    "            nfold = 5,\n",
    "            metrics='merror', early_stopping_rounds=50)\n",
    "\n",
    "number_of_est = cv.shape[0]\n",
    "estimator.set_params(n_estimators=number_of_est)\n",
    "\n",
    "\n",
    "\n",
    "params1 = {'min_child_weight':range(1,6,2),'max_depth':range(3,10,2)\n",
    "            }\n",
    "\n",
    "\n",
    "gd_search1 = GridSearchCV(\n",
    "                    estimator =estimator,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5,\n",
    "                    n_jobs = 1,\n",
    "                    param_grid = params1\n",
    "                    )\n",
    "gd_search1.fit(X_train,y_train)\n",
    "\n",
    "#gridsearch1.best_params_: min_child_weight=3, max_depth=5\n",
    "params2 = {'min_child_weight':[2,3,4],'max_depth':[4,5,6]}\n",
    "\n",
    "estimator.set_params(min_child_weight=gd_search1.best_params_['min_child_weight'],\n",
    "                     max_depth=gd_search1.best_params_['max_depth'])\n",
    "\n",
    "\n",
    "gd_search2 = GridSearchCV(\n",
    "                    estimator =estimator,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5,\n",
    "                    n_jobs = 1,\n",
    "                    param_grid = params2\n",
    "                    )\n",
    "gd_search2.fit(X_train,y_train)\n",
    "\n",
    "#gridsearch1.best_params_: min_child_weight=2, max_depth=6\n",
    "estimator.set_params(min_child_weight=gd_search2.best_params_['min_child_weight'],\n",
    "                     max_depth=gd_search2.best_params_['max_depth'])\n",
    "\n",
    "params3 = {'gamma':[i/100.0 + 2.0 for i in range(0,6)]}\n",
    "\n",
    "gd_search3 = GridSearchCV(\n",
    "                    estimator =estimator,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5,\n",
    "                    n_jobs = 1,\n",
    "                    param_grid = params3\n",
    "                    )\n",
    "gd_search3.fit(X_train,y_train)\n",
    "\n",
    "estimator.set_params(gamma=gd_search3.best_params_['gamma'])\n",
    "\n",
    "params4 = {'subsample': [i/10.0 for i in range(6,10)],\n",
    "                         'colsample_bytree': [i/10.0 for i in range(6,10)]}\n",
    "gd_search4 = GridSearchCV(\n",
    "                    estimator =estimator,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5,\n",
    "                    n_jobs = 1,\n",
    "                    param_grid = params4\n",
    "                    )\n",
    "gd_search4.fit(X_train,y_train)\n",
    "\n",
    "#optimal vals:'colsample_bytree': 0.8, 'subsample': 0.8\n",
    "#divided learning_rate by 10 and multiply n_estimators by 5\n",
    "#print(gd_search4.best_score_)\n",
    "\n",
    "estimator.set_params(n_estimators=300,\n",
    "                     learning_rate = 0.09)\n",
    "#0.7368661489264504 with n_estimators=300, learning_rate = 0.09\n",
    "\n",
    "\n",
    "res = cross_val_score(estimator, X=X_train, y=y_train, scoring=\"accuracy\",\n",
    "cv = KFold(shuffle=True, random_state=23333))\n",
    "\n",
    "#I use le2.inverse_transform to revert labels in y\n",
    "\n",
    "\n",
    "\n",
    "#print(\"### estimator parameters and cross val score###\")\n",
    "#print(estimator.get_params)\n",
    "#print(res.mean())\n",
    "#print(\"##########\")\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "final_result = estimator.predict(eval_train)\n",
    "\n",
    "final_result = le2.inverse_transform(final_result)\n",
    "#print(final_result)\n",
    "#print(np.shape(final_result))\n",
    "#print(\"######\")\n",
    "#print(np.shape(e_x6))\n",
    "\n",
    "np.savetxt(\"labels.txt\", final_result,fmt='%s')\n",
    "\n",
    "testest=np.loadtxt('labels.txt', dtype='str')\n",
    "#print(np.shape(testest))\n",
    "\n",
    "\n",
    "myfile=open(\"labels.txt\",\"r\")\n",
    "\n",
    "preds=myfile.read()\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "r=preds.split(\"\\n\")\n",
    "\n",
    "stuff=\"\\n\".join(r[:-1])\n",
    "\n",
    "myfile=open(\"labels.txt\",\"w+\")\n",
    "\n",
    "for i in range(len(stuff)):\n",
    "    myfile.write(stuff[i])\n",
    "myfile.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
